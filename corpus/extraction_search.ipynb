{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HITL_Workflow Iteration I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This iteration represents a first step of the scientific content creation workflow.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![setup](<setup.png>)\n",
    "![knowledge](<knowledge.png>)\n",
    "![publish](<publish.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell only the first time, to install the requiremenets.\n",
    "#pip install -r requirements.txt && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial step is to formulate a search query that aligns with the desired research objective. In this task, we can use a large language model (LLM) to either refine a research question, or extract relevant keywords that will be used in the process of querying scientific databases. \n",
    "\n",
    "The second step is inputing the search query that will be fed to different scraping models. This step represents the core of this iteration. \n",
    "\n",
    "The used tools in this Iteration are: \n",
    "-  **LLM** (Optional for formulating the search query)\n",
    "-  Modified [RESP](https://github.com/monk1337/resp) Arxiv-module \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formulated research question is to be given in the next cell. An example with the following research question is already given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we are just using keywords.\n",
    "papers_search_query = 'large language models for effective knowledge extraction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell calls the arxiv modul that scrapes 100 papers that resulted from inputing the search query to arxiv.org\n",
    "\n",
    "The result is saved in the variable \"arxiv_result\". This variable represents a dataframe with 3 columns, namely the title of the paper, link to the paper and a second link to download the pdf file of each respective paper.\n",
    "\n",
    "The cell also displays an overview of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.57s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>pdf_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explainable Biomedical Hypothesis Generation v...</td>\n",
       "      <td>https://arxiv.org/abs/2407.12888</td>\n",
       "      <td>https://arxiv.org/pdf/2407.12888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actionable Cyber Threat Intelligence using Kno...</td>\n",
       "      <td>https://arxiv.org/abs/2407.02528</td>\n",
       "      <td>https://arxiv.org/pdf/2407.02528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zero-shot Persuasive Chatbots with LLM-Generat...</td>\n",
       "      <td>https://arxiv.org/abs/2407.03585</td>\n",
       "      <td>https://arxiv.org/pdf/2407.03585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Extracting and Encoding: Leveraging Large Lang...</td>\n",
       "      <td>https://arxiv.org/abs/2407.01948</td>\n",
       "      <td>https://arxiv.org/pdf/2407.01948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BiosERC: Integrating Biography Speakers Suppor...</td>\n",
       "      <td>https://arxiv.org/abs/2407.04279</td>\n",
       "      <td>https://arxiv.org/pdf/2407.04279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Explainable Biomedical Hypothesis Generation v...   \n",
       "1  Actionable Cyber Threat Intelligence using Kno...   \n",
       "2  Zero-shot Persuasive Chatbots with LLM-Generat...   \n",
       "3  Extracting and Encoding: Leveraging Large Lang...   \n",
       "4  BiosERC: Integrating Biography Speakers Suppor...   \n",
       "\n",
       "                               link                          pdf_link  \n",
       "0  https://arxiv.org/abs/2407.12888  https://arxiv.org/pdf/2407.12888  \n",
       "1  https://arxiv.org/abs/2407.02528  https://arxiv.org/pdf/2407.02528  \n",
       "2  https://arxiv.org/abs/2407.03585  https://arxiv.org/pdf/2407.03585  \n",
       "3  https://arxiv.org/abs/2407.01948  https://arxiv.org/pdf/2407.01948  \n",
       "4  https://arxiv.org/abs/2407.04279  https://arxiv.org/pdf/2407.04279  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from arxiv_api import Arxiv\n",
    "ap           = Arxiv()\n",
    "arxiv_result = ap.arxiv(papers_search_query, max_pages = 1)\n",
    "arxiv_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell saves the titles and the respective pdf links to two variables, we would use the titles to save each downloaded pdf file to the name of the respective paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_links = arxiv_result[\"pdf_link\"]\n",
    "titles = arxiv_result[\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the next cell creates a directory called \"pdfs_corpus\", in which the pdf files will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import wget\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "\n",
    "download_dir = \"pdfs_corpus\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "invalid_char_re = re.compile(r'[<>:\"/\\\\|?*]')\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "\n",
    "    return invalid_char_re.sub('_', filename)\n",
    "\n",
    "def download_pdf(url, save_name):\n",
    "    try:\n",
    "        if not save_name.endswith(\".pdf\"):\n",
    "            save_name += \".pdf\"\n",
    "\n",
    "        save_name = sanitize_filename(save_name)\n",
    "        \n",
    "        save_path = os.path.join(download_dir, save_name)\n",
    "\n",
    "        encoded_url = quote(url, safe=\":/\")\n",
    "        \n",
    "        response = requests.get(encoded_url, stream=True)\n",
    "        response.raise_for_status() \n",
    "\n",
    "        if 'application/pdf' not in response.headers.get('Content-Type', ''):\n",
    "            raise ValueError(f\"URL does not point to a PDF: {url}\")\n",
    "        \n",
    "        with open(save_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this cell iterates through the dataframe and downloads/saves each paper under their name in the folder created in the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url, custom_name in zip(pdf_links, titles):\n",
    "   download_pdf(url, custom_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in this iteration creates a folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF to text conversion completed!\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "pdf_directory = \"pdfs_corpus\"\n",
    "text_directory = \"text_corpus\"\n",
    "\n",
    "os.makedirs(text_directory, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(pdf_directory):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_directory, filename)\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            full_text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                full_text += page.extract_text()\n",
    "        \n",
    "        text_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "        text_path = os.path.join(text_directory, text_filename)\n",
    "        \n",
    "        with open(text_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "            text_file.write(full_text)\n",
    "\n",
    "print(\"PDF to text conversion completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

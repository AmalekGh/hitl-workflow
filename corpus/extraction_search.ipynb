{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HITL_Workflow Iteration I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This iteration represents a first step of the scientific content creation workflow.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![setup](<setup.png>)\n",
    "![knowledge](<knowledge.png>)\n",
    "![publish](<publish.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial step is to formulate a search query that aligns with the desired research objective. In this task, we can use a large language model (LLM) to either refine a research question, or extract relevant keywords that will be used in the process of querying scientific databases. \n",
    "\n",
    "The second step is inputing the search query that will be fed to different scraping models. This step represents the core of this iteration. \n",
    "\n",
    "The used tools in this Iteration are: \n",
    "-  **LLM** (Optional for formulating the search query)\n",
    "-  Modified [RESP](https://github.com/monk1337/resp) Arxiv-module \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formulated research question is to be given in the next cell. An example with the following research question is already given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_search_query = 'How can large language models be utilized for effective knowledge extraction?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell calls the arxiv modul that scrapes 100 papers that resulted from inputing the search query to arxiv.org\n",
    "\n",
    "The result is saved in the variable \"arxiv_result\". This variable represents a dataframe with 3 columns, namely the title of the paper, link to the paper and a second link to download the pdf file of each respective paper.\n",
    "\n",
    "The cell also displays an overview of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arxiv_api import Arxiv\n",
    "ap           = Arxiv()\n",
    "arxiv_result = ap.arxiv(papers_search_query, max_pages = 1)\n",
    "arxiv_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell saves the titles and the respective pdf links to two variables, we would use the titles to save each downloaded pdf file to the name of the respective paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_links = arxiv_result[\"pdf_link\"]\n",
    "titles = arxiv_result[\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the next cell creates a directory called \"pdfs_corpus\", in which the pdf files will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import wget\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "\n",
    "download_dir = \"pdfs_corpus\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "invalid_char_re = re.compile(r'[<>:\"/\\\\|?*]')\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "\n",
    "    return invalid_char_re.sub('_', filename)\n",
    "\n",
    "def download_pdf(url, save_name):\n",
    "    try:\n",
    "        if not save_name.endswith(\".pdf\"):\n",
    "            save_name += \".pdf\"\n",
    "\n",
    "        save_name = sanitize_filename(save_name)\n",
    "        \n",
    "        save_path = os.path.join(download_dir, save_name)\n",
    "\n",
    "        encoded_url = quote(url, safe=\":/\")\n",
    "        \n",
    "        response = requests.get(encoded_url, stream=True)\n",
    "        response.raise_for_status() \n",
    "\n",
    "        if 'application/pdf' not in response.headers.get('Content-Type', ''):\n",
    "            raise ValueError(f\"URL does not point to a PDF: {url}\")\n",
    "        \n",
    "        with open(save_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this cell iterates through the dataframe and downloads/saves each paper under their name in the folder created in the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url, custom_name in zip(pdf_links, titles):\n",
    "   download_pdf(url, custom_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HITL-SCC_Workflow Iteration I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This iteration represents a first step of the Scientific Content Creation Workflow.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Scientific Content Creation Workflow represents a user interactive pipeline to systematically extract knowledge from a corpus of scientific literature. This should help the user have better insights into key contents of the corpus.\n",
    "\n",
    "This notebook provides a step-by-step, instruction based approach from setting up the corpus to extracting and representing knowledge relevant to the user.\n",
    "\n",
    "It is designed to support the user in the process of retrieving a set of relevant literature, and to convert the knowledge within to scientific knowledge graphs.\n",
    "\n",
    "This iterative process requires little to no programming prior knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![setup](<MA003.jpg>)\n",
    "![knowledge](<Frame9.jpg>)\n",
    "![publish](<Frame10.jpg>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell only the first time, to install the requiremenets.\n",
    "#pip install -r requirements.txt && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial step is to formulate a search query that aligns with the desired research objective. In this task, we can use a large language model (LLM) to either refine a research question, or extract relevant keywords that will be used in the process of querying scientific databases. \n",
    "\n",
    "The second step is inputing the search query that will be fed to different scraping models. This step represents the core of this iteration. \n",
    "\n",
    "The used tools in this Iteration are: \n",
    "-  **LLM** (Optional for formulating the search query)\n",
    "-  Modified [RESP](https://github.com/monk1337/resp) Arxiv-module \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formulated research question is to be given in the next cell. An example with the following research question is already given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we are just using keywords.\n",
    "papers_search_query = 'large language models for effective knowledge extraction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell calls the arxiv modul that scrapes 100 papers that resulted from inputing the search query to arxiv.org\n",
    "\n",
    "The result is saved in the variable \"arxiv_result\". This variable represents a dataframe with 3 columns, namely the title of the paper, link to the paper and a second link to download the pdf file of each respective paper.\n",
    "\n",
    "The cell also displays an overview of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.45s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from util.arxiv_api import Arxiv\n",
    "ap           = Arxiv()\n",
    "arxiv_result = ap.arxiv(papers_search_query,50, max_pages = 1)\n",
    "arxiv_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell saves the titles and the respective pdf links to two variables, we would use the titles to save each downloaded pdf file to the name of the respective paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_links = arxiv_result[\"pdf_link\"]\n",
    "titles = arxiv_result[\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the next cell creates a directory called \"pdfs_corpus\", in which the pdf files will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import wget\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "\n",
    "download_dir = \"pdfs_corpus\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "invalid_char_re = re.compile(r'[<>:\"/\\\\|?*]')\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "\n",
    "    return invalid_char_re.sub('_', filename)\n",
    "\n",
    "def download_pdf(url, save_name):\n",
    "    try:\n",
    "        if not save_name.endswith(\".pdf\"):\n",
    "            save_name += \".pdf\"\n",
    "\n",
    "        save_name = sanitize_filename(save_name)\n",
    "        \n",
    "        save_path = os.path.join(download_dir, save_name)\n",
    "\n",
    "        encoded_url = quote(url, safe=\":/\")\n",
    "        \n",
    "        response = requests.get(encoded_url, stream=True)\n",
    "        response.raise_for_status() \n",
    "\n",
    "        if 'application/pdf' not in response.headers.get('Content-Type', ''):\n",
    "            raise ValueError(f\"URL does not point to a PDF: {url}\")\n",
    "        \n",
    "        with open(save_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this cell iterates through the dataframe and downloads/saves each paper under their name in the folder created in the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url, custom_name in zip(pdf_links, titles):\n",
    "   download_pdf(url, custom_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in this iteration creates the \"text_corpus\". \n",
    "Running the next cell creates a corpus of text files from the downloaded pdf files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF to text conversion completed!\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "pdf_directory = \"pdfs_corpus\"\n",
    "text_directory = \"text_corpus\"\n",
    "\n",
    "os.makedirs(text_directory, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(pdf_directory):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_directory, filename)\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            full_text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                full_text += page.extract_text()\n",
    "        \n",
    "        text_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "        text_path = os.path.join(text_directory, text_filename)\n",
    "        \n",
    "        with open(text_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "            text_file.write(full_text)\n",
    "\n",
    "print(\"PDF to text conversion completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

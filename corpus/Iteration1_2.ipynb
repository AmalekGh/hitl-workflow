{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HITL-SCC_Workflow Iteration I_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step of the Iteration represents the processing of the extracted text and preparing the input for the Large language model.\n",
    "This step is considered the core of the RAG application (Retrieval augmented generation). Here, we use the context of the paper to leverage the ability\n",
    "of the models to extract relevant parts of the paper and to perform context-based analysis. This part also creates a data model based on user input.\n",
    "\n",
    "![rag_pipeline](<media/RAG_pipeline.jpg>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We list the tools used in this part: \n",
    "-  **Ollama**\n",
    "\n",
    "Ollama is a service that provides easy access to large language Models and other tools needed for the embedding, computing and generating text.\n",
    "It allows us in this workflow to communicate with the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Defining Data Model**\n",
    "\n",
    "This steps allows the user to define the data model. \n",
    "A data model in this context is a structured set of properties that should serve as an input in order to communicate with the corpus. \n",
    "The properties can include multiple parts of a paper as well as specific values relevant to the user. \n",
    "\n",
    "For now, we define a data model to be a list that contains one (or multiple sets) of the following properties: \n",
    "- Title\n",
    "- Theme\n",
    "- Keywords\n",
    "- Task\n",
    "- Evaluation Approach\n",
    "- Future Directions\n",
    "- Theories\n",
    "- Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b591371730034e85bae95cf394999777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Define an element from the data model here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "input_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Define an element from the data model here',\n",
    "    disabled=False\n",
    ")\n",
    "def save_input(change):\n",
    "    global data_element\n",
    "    data_element = change['new']\n",
    "\n",
    "input_widget.observe(save_input, names='value')\n",
    "\n",
    "display(input_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Text chunking**\n",
    "\n",
    "In this part, we compute the text of the documents in order to create a meaningful start point for the communication with the documents.\n",
    "\n",
    "To better identify information present in the text, a semantic chunking method is used in order to create smaller parts of the text. \n",
    "\n",
    "The result of this step is creating semantically conntected units of text that are easier to process.\n",
    "\n",
    "\n",
    "![rag_pipeline](<media/semantic.jpg>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from embedding.document_util import DocumentUtil\n",
    "from chunking.chunks_util import ChunksUtil\n",
    "\n",
    "\n",
    "test_document = DocumentUtil.get_text_from_document(DocumentUtil, './')\n",
    "chunked_text = ChunksUtil.compute_chunks(ChunksUtil, test_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Similarity search & Prompting**\n",
    "\n",
    "This parts consists of retrieving the related information to the provided data model. \n",
    "In this step, we search for the top k chunks of text that result of a vector search between each chunk and the respective query created out of the data model.\n",
    "\n",
    "The top k chunks are then used in the prompt given to the large language model in order to provide context in this application.\n",
    "\n",
    "Running the code cell below outputs the LLM-generated text, which represent the identified data out of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPMDS, EMMSAD, fCM, Workflow pattern, Exception handling, Recovery measures, Compensation, Rollback, Faineance, SMile2 project, Fragment-level implementation, NE, DE, MA, NT.\n"
     ]
    }
   ],
   "source": [
    "from embedding.embedding_util import EmbeddingUtil \n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "from llms.llm_util import LlmUtil\n",
    "from prompting.prompts import Prompts\n",
    "\n",
    "top_k_chunks = EmbeddingUtil.compute_top_k_ollama(EmbeddingUtil, chunked_text, 'The Keywords of this paper are')\n",
    "prompt = Prompts.get_prompt_2(Prompts, top_k_chunks, 'What are the Keywords of this paper?')\n",
    "response = LlmUtil.prompt_ollama(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

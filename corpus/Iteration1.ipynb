{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HITL-SCC_Workflow Iteration I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iteration represents the first step of the Scientific Content Creation Workflow, whiche represents a user interactive pipeline to systematically extract knowledge from a corpus of scientific literature. This should help the user have better insights into key contents of the corpus.\n",
    "\n",
    "This notebook provides a step-by-step, instruction based approach from setting up the corpus to extracting and representing knowledge relevant to the user.\n",
    "\n",
    "The first task is to support the user in the process of retrieving a set of relevant literature, and to better represent the knowledge for the user.\n",
    "\n",
    "This iterative process requires little to no programming prior knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![setup](<media/MA003.jpg>)\n",
    "![knowledge](<media/Frame9.jpg>)\n",
    "![publish](<media/Frame10.jpg>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the described process, this notebook runs tools and functions that are specifically implemented to query and scrape different digital libraries. A requirements file (requirements.txt) is predefined to install all necessary packages. \n",
    "\n",
    "This requires a Jupyter environment that runs any version of python 3.\n",
    "\n",
    "In order to install the different packages, we only need to run the next cell one time. If you already run it once on your machine, just ignore it and don't run it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two approaches are developed to extract a corpus of PDFs. One is for the case of not having a set of scientfic literature, the second is for the case of having one. Note that either step 1. or step 2. should be used to extract a corpus of PDFs, and not both. If you already have a set of papers in a Zotero collection, please skip to step 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Corpus mining**\n",
    "\n",
    "The initial step is to formulate a search query that aligns with the desired research objective. In this task, we can use a large language model (LLM) to extract relevant keywords that will be used in the process of querying scientific databases. \n",
    "\n",
    "The second step is inputing the search query that will be fed to different scraping models. This step represents the core of this iteration. \n",
    "\n",
    "The used tools in this step are: \n",
    "-  **LLM** (Optional for formulating the search query)\n",
    "-  Modified **[RESP](https://github.com/monk1337/resp)** Arxiv-module \n",
    "- **Semantic Scholar API**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user formulates a search query and copies this in the space between the single quotation marks below in the next cell. We use the variable named **papers_search_query**\n",
    "\n",
    "An example that can be used as a search query is: *large language models for effective knowledge extraction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd4f232a4874bd3a7f02f86a856bf04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Your search query here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "input_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Your search query here',\n",
    "    disabled=False\n",
    ")\n",
    "def save_input(change):\n",
    "    global papers_search_query\n",
    "    papers_search_query = change['new']\n",
    "\n",
    "input_widget.observe(save_input, names='value')\n",
    "\n",
    "display(input_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models for knowledge extraction\n"
     ]
    }
   ],
   "source": [
    "print(papers_search_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell allows the user to predefine the size limit of the corpus to be created. The variable **limit** holds to maximum size of papers to be downloaded from each source. \n",
    "\n",
    "Note that many search results don't include an open acess to PDFs.\n",
    "\n",
    "The next cell has 50 as a predefined value ( 50 pdf as a maximum from each different source ).\n",
    "\n",
    "The user is able to modify this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a896a476dd164da584532b455af9d9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Documents limit')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Documents limit',\n",
    "    disabled=False\n",
    ")\n",
    "def save_input(change):\n",
    "    global limit\n",
    "    limit = change['new']\n",
    "\n",
    "# Attach the function to the Text widget\n",
    "input_widget.observe(save_input, names='value')\n",
    "\n",
    "# Display the widget\n",
    "display(input_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, is to define the sources of our corpus. Multiple digital libraries and scientific databases can be accessed and queried. \n",
    "\n",
    "Here we create a list of sources, that the user can adapt. Note that the names of the sources are given between quotations and separated by a coma \",\" as in the example below. The elements in the list are responsible of specifying which and how many sources we take in consieration.\n",
    "\n",
    "**Note** This version only supports querying **Arxiv** and **Semantic Scholar**. Later version will include further sources.\n",
    "\n",
    "Current possible entries for the list: \n",
    "- \"Arxiv\"\n",
    "- \"Semantic Scholar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58e1e7659ad4838aee0e41f3ed67c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Your source here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Your source here',\n",
    "    disabled=False\n",
    ")\n",
    "def save_input(change):\n",
    "    global sources\n",
    "    sources = change['new']\n",
    "\n",
    "input_widget.observe(save_input, names='value')\n",
    "\n",
    "display(input_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying Semantic Scholar...\n",
      "Number of papers with PDF: 18\n",
      "Semantic Scholar was successfully queried...\n"
     ]
    }
   ],
   "source": [
    "from util.arxiv_api import Arxiv\n",
    "from util.semanticscholar_util import SemanticScholar\n",
    "\n",
    "for source in sources:\n",
    "    if source == \"Arxiv\":\n",
    "        arxiv_instance = Arxiv()\n",
    "        arxiv_instance.download_pdf(papers_search_query, limit)\n",
    "            \n",
    "    elif source == \"Semantic Scholar\":\n",
    "        semanticscholar_instance = SemanticScholar()\n",
    "        semanticscholar_instance.download_pdfs(papers_search_query, limit)\n",
    "    else:\n",
    "        print(\"Unknown Identifier specified in the sources\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Corpus extraction from Zotero**\n",
    "\n",
    "This approach is for the case of having a Zotero collections that we want to investigate. This will result in creating a corpus of PDFs locally saved on the users local machine. For this, we use Zotero's API. \n",
    "\n",
    "This step requires a unique Zotero API Key, the library ID, the library type, and the collection's ID. This information can be found/set up in your personal zotero account.\n",
    "\n",
    "The used tools in this step are: \n",
    "- **Zotero** and **Zotero API**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "Paper has no accessible PDF.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Paper has no accessible PDF.\n",
      "Download completed. Found documents were downloaded\n"
     ]
    }
   ],
   "source": [
    "from pyzotero import zotero\n",
    "import os\n",
    "from util.zotero_util import ZoteroUtil\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv('ZOTERO_API_KEY')\n",
    "LIBRARY_ID = os.getenv('LIBRARY_ID')\n",
    "LIBRARY_TYPE = os.getenv('LIBRARY_TYPE')\n",
    "COLLECTIONS = os.getenv('COLLECTION')\n",
    "\n",
    "\n",
    "zot = zotero.Zotero(LIBRARY_ID, LIBRARY_TYPE, API_KEY)\n",
    "\n",
    "download_directory = 'zotero_pdfs1'\n",
    "os.makedirs(download_directory, exist_ok=True)\n",
    "\n",
    "items = zot.collection_items(COLLECTIONS)\n",
    "for item in items:\n",
    "    if 'url' in item['data']:\n",
    "        ZoteroUtil.download_pdf(item['data']['url'], item['data']['key'])\n",
    "        \n",
    "        \n",
    "print(\"Download completed. Found documents were downloaded\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. From PDF to Text** \n",
    "\n",
    "This step consists of turning the pdf files into textual files that can be treated and transfered as input of the later steps. \n",
    "\n",
    "We start from a corpus of pdf files and aim to have a folder filled with files with the extension (.txt).\n",
    "\n",
    "The used tools in this step are: \n",
    "- **PDF Plumber**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF to text conversion completed!\n",
      "Converted and saved 240307541v2.pdf to 240307541v2.txt\n"
     ]
    }
   ],
   "source": [
    "from util.pdf_util import PdfUtil\n",
    "\n",
    "folder_path = 'one_paper'\n",
    "output_path = 'text_files'\n",
    "\n",
    "\n",
    "# Convert all PDFs in the folder\n",
    "PdfUtil.convert_pdfs_in_folder(PdfUtil, folder_path, output_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

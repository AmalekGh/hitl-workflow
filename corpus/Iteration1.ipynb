{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HITL-SCC_Workflow Iteration I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iteration represents the first step of the Scientific Content Creation Workflow, whiche represents a user interactive pipeline to systematically extract knowledge from a corpus of scientific literature. This should help the user have better insights into key contents of the corpus.\n",
    "\n",
    "This notebook provides a step-by-step, instruction based approach from setting up the corpus to extracting and representing knowledge relevant to the user.\n",
    "\n",
    "The first task is to support the user in the process of retrieving a set of relevant literature, and to better represent the knowledge for the user.\n",
    "\n",
    "This iterative process requires little to no programming prior knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![setup](<media/MA003.jpg>)\n",
    "![knowledge](<media/Frame9.jpg>)\n",
    "![publish](<media/Frame10.jpg>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the described process, this notebook runs tools and functions that are specifically implemented to query and scrape different digital libraries. A requirements file (requirements.txt) is predefined to install all necessary packages. \n",
    "\n",
    "This requires a Jupyter environment that runs any version of python 3.\n",
    "\n",
    "In order to install the different packages, we only need to run the next cell one time. If you already run it once on your machine, just ignore it and don't run it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two approaches are developed to extract a corpus of PDFs. One is for the case of not having a set of scientfic literature, the second is for the case of having one. Note that either step 1. or step 2. should be used to extract a corpus of PDFs, and not both. If you already have a set of papers in a Zotero collection, please skip to step 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Corpus mining**\n",
    "\n",
    "The initial step is to formulate a search query that aligns with the desired research objective. In this task, we can use a large language model (LLM) to extract relevant keywords that will be used in the process of querying scientific databases. \n",
    "\n",
    "The second step is inputing the search query that will be fed to different scraping models. This step represents the core of this iteration. \n",
    "\n",
    "The used tools in this step are: \n",
    "-  **LLM** (Optional for formulating the search query)\n",
    "-  Modified **[RESP](https://github.com/monk1337/resp)** Arxiv-module \n",
    "- **Semantic Scholar API**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user formulates a search query and copies this in the space between the single quotation marks below in the next cell. We use the variable named **papers_search_query**\n",
    "\n",
    "An example that can be used as a search query is: *large language models for effective knowledge extraction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_search_query = 'large language models for effective knowledge extraction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell allows the user to predefine the size limit of the corpus to be created. The variable **limit** holds to maximum size of papers to be downloaded from each source. \n",
    "\n",
    "Note that many search results don't include an open acess to PDFs.\n",
    "\n",
    "The next cell has 50 as a predefined value ( 50 pdf as a maximum from each different source ).\n",
    "\n",
    "The user is able to modify this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, is to define the sources of our corpus. Multiple digital libraries and scientific databases can be accessed and queried. \n",
    "\n",
    "Here we create a list of sources, that the user can adapt. Note that the names of the sources are given between quotations and separated by a coma \",\" as in the example below. The elements in the list are responsible of specifying which and how many sources we take in consieration.\n",
    "\n",
    "**Note** This version only supports querying **Arxiv** and **Semantic Scholar**. Later version will include further sources.\n",
    "\n",
    "Current possible entries for the list: \n",
    "- \"Arxiv\"\n",
    "- \"Semantic Scholar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [\"Semantic Scholar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying Semantic Scholar...\n",
      "Number of papers with PDF: 18\n",
      "Semantic Scholar was successfully queried...\n"
     ]
    }
   ],
   "source": [
    "from util.arxiv_api import Arxiv\n",
    "from util.semanticscholar_util import SemanticScholar\n",
    "\n",
    "for source in sources:\n",
    "    if source == \"Arxiv\":\n",
    "        arxiv_instance = Arxiv()\n",
    "        arxiv_instance.download_pdf(papers_search_query, limit)\n",
    "            \n",
    "    elif source == \"Semantic Scholar\":\n",
    "        semanticscholar_instance = SemanticScholar()\n",
    "        semanticscholar_instance.download_pdfs(papers_search_query, limit)\n",
    "    else:\n",
    "        print(\"Unknown Identifier specified in the sources\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Corpus extraction from Zotero**\n",
    "\n",
    "This approach is for the case of having a Zotero collections that we want to investigate. This will result in creating a corpus of PDFs locally saved on the users local machine. For this, we use Zotero's API. \n",
    "\n",
    "This step requires a unique Zotero API Key, the library ID, the library type, and the collection's ID. This information can be found/set up in your personal zotero account.\n",
    "\n",
    "The used tools in this step are: \n",
    "- **Zotero** and **Zotero API**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF failed to download.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF Downloaded.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "PDF failed to download.\n",
      "Download completed.\n"
     ]
    }
   ],
   "source": [
    "from pyzotero import zotero\n",
    "import os\n",
    "from util.zotero_util import ZoteroUtil\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv('ZOTERO_API_KEY')\n",
    "LIBRARY_ID = os.getenv('LIBRARY_ID')\n",
    "LIBRARY_TYPE = os.getenv('LIBRARY_TYPE')\n",
    "COLLECTIONS = os.getenv('COLLECTION')\n",
    "\n",
    "\n",
    "zot = zotero.Zotero(LIBRARY_ID, LIBRARY_TYPE, API_KEY)\n",
    "\n",
    "download_directory = 'zotero_pdfs'\n",
    "os.makedirs(download_directory, exist_ok=True)\n",
    "\n",
    "items = zot.collection_items(COLLECTIONS)\n",
    "for item in items:\n",
    "    if 'url' in item['data']:\n",
    "        ZoteroUtil.download_pdf(item['data']['url'], item['data']['key'])\n",
    "        \n",
    "        \n",
    "print(\"Download completed. Found documents were downloaded\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. From PDF to Text** \n",
    "\n",
    "This step consists of turning the pdf files into textual files that can be treated and transfered as input of the later steps. \n",
    "\n",
    "We start from a corpus of pdf files and aim to have a folder filled with files with the extension (.txt).\n",
    "\n",
    "The used tools in this step are: \n",
    "- **PDF Plumber**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 2UZS965Y.pdf\n",
      "Extracted text from 4KKE293P.pdf\n",
      "Extracted text from 4XTLX385.pdf\n",
      "Extracted text from 56FCMXLU.pdf\n",
      "Extracted text from 5F2YMTBS.pdf\n",
      "Extracted text from 5R8BBVAT.pdf\n",
      "Extracted text from 6ZNP9N2R.pdf\n",
      "Extracted text from 7MWFJGJ8.pdf\n",
      "Extracted text from 7NK8VXTJ.pdf\n",
      "Extracted text from 9P4JSU6X.pdf\n",
      "Extracted text from B7VCHQ4F.pdf\n",
      "Extracted text from BF5URTZH.pdf\n",
      "Extracted text from BJBNMJ9E.pdf\n",
      "Extracted text from CDXX3MWS.pdf\n",
      "Extracted text from CE55GFUT.pdf\n",
      "Extracted text from F95SXDZF.pdf\n",
      "Extracted text from FWSAQ63Q.pdf\n",
      "Extracted text from GXFG7YXK.pdf\n",
      "Extracted text from GYZHZKLY.pdf\n",
      "Extracted text from HAKQ2XWB.pdf\n",
      "Extracted text from LCQTNCZL.pdf\n",
      "Extracted text from LPHHVYES.pdf\n",
      "Extracted text from N8IKZ2NB.pdf\n",
      "Extracted text from NGKVNPT5.pdf\n",
      "Extracted text from NZT9XNNS.pdf\n",
      "Extracted text from P64IC447.pdf\n",
      "Extracted text from PZU6LCF9.pdf\n",
      "Extracted text from Q6LTG6DY.pdf\n",
      "Extracted text from QB4YGC8U.pdf\n",
      "Extracted text from QEBUADPH.pdf\n",
      "Extracted text from QX24L47J.pdf\n",
      "Extracted text from RIGL7W83.pdf\n",
      "Extracted text from SHSM9MS7.pdf\n",
      "Extracted text from T822ESXR.pdf\n",
      "Extracted text from UT9ZZLNL.pdf\n",
      "Extracted text from XF4FG3VM.pdf\n",
      "Extracted text from Y2YM3XUZ.pdf\n",
      "Extracted text from Z6UBL9M9.pdf\n",
      "PDF to text conversion completed!\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "pdf_directory = \"zotero_pdfs\"\n",
    "text_directory = \"text_corpus\"\n",
    "\n",
    "os.makedirs(text_directory, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(pdf_directory):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_directory, filename)\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                full_text = \"\"\n",
    "                for page in pdf.pages:\n",
    "                    try:\n",
    "                        text = page.extract_text()\n",
    "                        if text:\n",
    "                            full_text += text\n",
    "                    except Exception as e:\n",
    "                            full_text+=\"\"\n",
    "                \n",
    "                text_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "                text_path = os.path.join(text_directory, text_filename)\n",
    "                \n",
    "                with open(text_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "                    text_file.write(full_text)\n",
    "                \n",
    "                print(f\"Extracted text from {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {filename}: {e}\")\n",
    "\n",
    "print(\"PDF to text conversion completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
